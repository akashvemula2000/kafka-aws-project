# Kafka-AWS IoT Smart City Streaming Pipeline ðŸš€

[![Python](https://img.shields.io/badge/Python-3.9+-blue)](https://www.python.org/)
[![Kafka](https://img.shields.io/badge/Kafka-Apache%20Kafka-brightgreen)](https://kafka.apache.org/)
[![AWS](https://img.shields.io/badge/AWS-S3%20%7C%20Glue%20%7C%20Athena-orange)](https://aws.amazon.com/)
[![Spark](https://img.shields.io/badge/Spark-3.5.0-orange)](https://spark.apache.org/)

This project simulates real-time Smart City IoT data, ingests multiple data streams into Kafka, processes them using Apache Spark Structured Streaming, and stores processed data in AWS S3 for further analytics using AWS Glue and Athena. The system demonstrates an end-to-end scalable data pipeline architecture.

## Tech Stack

- Apache Kafka (Bitnami Docker)
- Apache Spark Structured Streaming (Bitnami Docker)
- AWS S3 (Data Lake Storage)
- AWS Glue (Data Catalog)
- AWS Athena (Query Engine)
- Docker Compose (Container Orchestration)
- Python (Kafka Producers & Spark Consumers)
- Pandas, Boto3, Folium (Data Processing & Visualization)

## Prerequisites

- Docker and Docker Compose
- Python 3.9+
- Virtual environment (recommended)
- AWS Account with appropriate permissions for S3, Glue, and Athena

## Project Structure

The main components of the project are organized as follows:

```
Kafka-IoT/
â”œâ”€â”€ Consumer-spark.py     
â”œâ”€â”€ IoT_Travel.py         
â”œâ”€â”€ Weather.py           
â”œâ”€â”€ docker-compose.yml   
â”œâ”€â”€ requirements.txt     
â”œâ”€â”€ reset_kafka_topics.sh
â”œâ”€â”€ config.py            
â””â”€â”€ data/                
```

## ðŸš€ Quick Start

1. Create and activate a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Start the Docker containers:
```bash
docker-compose up -d
```

## Services

The following services are launched via Docker Compose:
- Zookeeper (port 2181)
- Kafka (port 9092)
- Spark Master (ports 8080, 7077)
- Spark Workers (ports 8081, 8082)

## Usage

1. Start the Kafka producer:
```bash
python IoT_Travel.py
```

2. Run Spark Structured Streaming:
   - Local run:
   ```bash
   python Consumer-spark.py
   ```
   - Docker Spark Master run:
   ```bash
   docker exec -it spark-master spark-submit /app/Consumer-spark.py
   ```

## ðŸ“Š End-to-End Data Flow

1. IoT data is generated and sent to Kafka topics
2. Spark processes the streaming data
3. Processed data is stored in AWS S3 as Parquet files
4. AWS Glue Crawler automatically catalogs the data
5. Data becomes queryable through AWS Athena

## AWS Glue Integration

- AWS Glue crawler creates external tables
- Excludes: **/_spark_metadata/**
- Data stored as: s3://kafka-iot-project/data/{topic_name}/

## AWS Athena Integration

The processed data is automatically loaded into AWS Athena for querying. The data is organized in the following tables:

- `vehicle_data` - Vehicle information and location data
- `gps_data` - GPS tracking information
- `traffic_data` - Traffic camera data
- `weather_data` - Weather conditions and metrics
- `emergency_data` - Emergency incident reports

### Example Athena Queries

```sql
-- Get average speed by vehicle type
SELECT vehicle_type, AVG(speed) as avg_speed
FROM gps_data
GROUP BY vehicle_type;

-- Find emergency incidents in a specific area
SELECT *
FROM emergency_data
WHERE location LIKE '%32.7157%'
AND status = 'Active';

-- Get weather conditions during high-traffic periods
SELECT w.weather_condition, COUNT(*) as incident_count
FROM weather_data w
JOIN traffic_data t ON w.timestamp = t.timestamp
GROUP BY w.weather_condition;
```

## Spark UI

- Spark Master UI: http://localhost:8080
- Spark Worker 1 UI: http://localhost:8081
- Spark Worker 2 UI: http://localhost:8082

## Kafka CLI Reference

A reference file `Kafka CLI Commands` is included in the project for common Kafka operations.

## Data Processing

The project processes IoT data streams in real-time:
1. Data is produced by `IoT_Travel.py`
2. Kafka brokers handle the message streaming
3. Spark processes the data in a distributed manner
4. Results are stored in AWS S3 and made queryable through Athena

## License
MIT License

## Contact
Akash Vemula  
GitHub: [akashvemula2000](https://github.com/akashvemula2000)  
LinkedIn: [https://www.linkedin.com/in/akashvemula2000](https://www.linkedin.com/in/akashvemula2000) 